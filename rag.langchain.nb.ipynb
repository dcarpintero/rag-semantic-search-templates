{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Template with LangChain\n",
    "\n",
    "- Inference requires a [Groq](https://groq.com/) API KEY (free for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q langchain langchain-community langchain-huggingface langchain_groq sentence-transformers faiss-cpu bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build up Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the document(s) from web url's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader([\"https://ai.meta.com/blog/meta-llama-3-1/\",\n",
    "                        \"https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md\"])\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain offers various text splitters, with the `RecursiveCharacterTextSplitter` being a recommended choice for generic text. This splitter is intended to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the second and third chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def md(s):\n",
    "    display(Markdown(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Our approachResearchProduct experiencesLlamaBlogTry Meta AILarge Language ModelIntroducing Llama 3.1: Our most capable models to dateJuly 23, 2024•15 minute readTakeaways:Meta is committed to openly accessible AI. Read Mark Zuckerberg’s letter detailing why open source is good for developers, good for Meta, and good for the world.Bringing open intelligence to all, our latest models expand context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first frontier-level open source AI model.Llama 3.1 405B is in a class of its own, with unmatched flexibility, control, and state-of-the-art capabilities that rival the best closed source models. Our new model will enable the community to unlock new workflows, such as synthetic data generation and model distillation.We’re continuing to build out Llama to be a system by providing more components that work"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "md(splits[1].page_content)\n",
    "md(splits[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is indeed an overlap among those chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "and context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first fro"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "md(splits[1].page_content[-100:])\n",
    "md(splits[2].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Transformation, and Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the documents into a vector storage with an open-source embedding model. In this example we use FAISS, which is highly optimized for large-scale datasets and GPU acceleration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "db = FAISS.from_documents(documents = splits, \n",
    "                          embedding = HuggingFaceEmbeddings(model_name=embedding_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "GROQ_API_TOKEN = getpass()\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will use Llama3-8b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "chat_history = []\n",
    "chain = ConversationalRetrievalChain.from_llm(llm,\n",
    "                                              db.as_retriever(),\n",
    "                                              return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q&A with Source Citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the provided context, the context length in Llama 3.1 405B is 128K."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_query = \"how long is the context length in Llama 3.1 405B?\"\n",
    "llm_output = chain.invoke({\"question\": user_query, \"chat_history\": chat_history})\n",
    "\n",
    "# the answer should be 128k\n",
    "md(llm_output['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that LangChain includes the sources in the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md', 'title': 'llama-models/models/llama3_1/MODEL_CARD.md at main · meta-llama/llama-models · GitHub', 'description': 'Utilities intended for use with Llama models. Contribute to meta-llama/llama-models development by creating an account on GitHub.', 'language': 'en'}, page_content='Language\\n\\nLlama 3.1 8B Instruct\\n\\nLlama 3.1 70B Instruct\\n\\nLlama 3.1 405B Instruct\\n\\n\\n\\nGeneral\\n\\nMMLU (5-shot, macro_avg/acc)\\n\\nPortuguese\\n   \\n62.12\\n   \\n80.13\\n   \\n84.95\\n   \\n\\n\\nSpanish\\n   \\n62.45\\n   \\n80.05\\n   \\n85.08\\n   \\n\\n\\nItalian\\n   \\n61.63\\n   \\n80.4\\n   \\n85.04\\n   \\n\\n\\nGerman\\n   \\n60.59\\n   \\n79.27\\n   \\n84.36\\n   \\n\\n\\nFrench\\n   \\n62.34\\n   \\n79.82\\n   \\n84.66\\n   \\n\\n\\nHindi\\n   \\n50.88\\n   \\n74.52\\n   \\n80.31\\n   \\n\\n\\nThai\\n   \\n50.32\\n   \\n72.95\\n   \\n78.21'),\n",
       " Document(metadata={'source': 'https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md', 'title': 'llama-models/models/llama3_1/MODEL_CARD.md at main · meta-llama/llama-models · GitHub', 'description': 'Utilities intended for use with Llama models. Contribute to meta-llama/llama-models development by creating an account on GitHub.', 'language': 'en'}, page_content='Metric\\n\\nLlama 3 8B Instruct\\n\\nLlama 3.1 8B Instruct\\n\\nLlama 3 70B Instruct\\n\\nLlama 3.1 70B Instruct\\n\\nLlama 3.1 405B Instruct\\n\\n\\n\\nGeneral\\n   \\nMMLU\\n   \\n5\\n   \\nmacro_avg/acc\\n   \\n68.5\\n   \\n69.4\\n   \\n82.0\\n   \\n83.6\\n   \\n87.3\\n   \\n\\n\\nMMLU (CoT)\\n   \\n0\\n   \\nmacro_avg/acc\\n   \\n65.3\\n   \\n73.0\\n   \\n80.9\\n   \\n86.0\\n   \\n88.6\\n   \\n\\n\\nMMLU-Pro (CoT)\\n   \\n5\\n   \\nmacro_avg/acc\\n   \\n45.5\\n   \\n48.3\\n   \\n63.4\\n   \\n66.4\\n   \\n73.3\\n   \\n\\n\\nIFEval\\n   \\n\\n\\n\\n\\n76.8\\n   \\n80.4\\n   \\n82.9\\n   \\n87.5\\n   \\n88.6'),\n",
       " Document(metadata={'source': 'https://ai.meta.com/blog/meta-llama-3-1/', 'title': 'Introducing Llama 3.1: Our most capable models to date', 'description': 'Bringing open intelligence to all, our latest models expand context length, add support across eight languages, and include Meta Llama 3.1 405B— the...', 'language': 'en'}, page_content='context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first frontier-level open source AI model.Llama 3.1 405B is in a class of its own, with unmatched flexibility, control, and state-of-the-art capabilities that rival the best closed source models. Our new model will enable the community to unlock new workflows, such as synthetic data generation and model distillation.We’re continuing to build out Llama to be a system by providing more components that work'),\n",
       " Document(metadata={'source': 'https://ai.meta.com/blog/meta-llama-3-1/', 'title': 'Introducing Llama 3.1: Our most capable models to date', 'description': 'Bringing open intelligence to all, our latest models expand context length, add support across eight languages, and include Meta Llama 3.1 405B— the...', 'language': 'en'}, page_content='Our approachResearchProduct experiencesLlamaBlogTry Meta AILarge Language ModelIntroducing Llama 3.1: Our most capable models to dateJuly 23, 2024•15 minute readTakeaways:Meta is committed to openly accessible AI. Read Mark Zuckerberg’s letter detailing why open source is good for developers, good for Meta, and good for the world.Bringing open intelligence to all, our latest models expand context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_output['source_documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first source includes indeed the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Language\n",
       "\n",
       "Llama 3.1 8B Instruct\n",
       "\n",
       "Llama 3.1 70B Instruct\n",
       "\n",
       "Llama 3.1 405B Instruct\n",
       "\n",
       "\n",
       "\n",
       "General\n",
       "\n",
       "MMLU (5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "md(llm_output['source_documents'][0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Follow-up Question with Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [(user_query, llm_output[\"answer\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the text, the context length in the 8B model is 128K."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_query = \"what about the 8b model?\"\n",
    "llm_output = chain.invoke({\"question\": user_query, \"chat_history\": chat_history})\n",
    "md(llm_output['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Follow-up Question with *without* Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The text does not mention the \"8b model\". It does mention quantizing the 405B model from 16-bit (BF16) to 8-bit (FP8) numerics, but it does not mention an \"8b model\" specifically."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_query = \"what about the 8b model?\"\n",
    "llm_output = chain.invoke({\"question\": user_query, \"chat_history\": []})\n",
    "md(llm_output['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without chat history, the model appears to just retrieve passages that approximate the semantic meaning of the word 'model' contained in the user question, but is not able to retrieve information about the context length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Introducing Llama 3.1: Our most capable models to date"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "this blog post.)While this is our biggest model yet, we believe there’s still plenty of new ground to explore in the future, including more device-friendly sizes, additional modalities, and more investment at the agent platform layer.As always, we look forward to seeing all the amazing products and experiences the community will build with these models.This work was supported by our partners across the AI community. We’d like to thank and acknowledge (in alphabetical order): Accenture, Amazon"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "parameter model to improve the post-training quality of our smaller models.To support large-scale production inference for a model at the scale of the 405B, we quantized our models from 16-bit (BF16) to 8-bit (FP8) numerics, effectively lowering the compute requirements needed and allowing the model to run within a single server node.Instruction and chat fine-tuningWith Llama 3.1 405B, we strove to improve the helpfulness, quality, and detailed instruction-following capability of the model in"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "translation. With the release of the 405B model, we’re poised to supercharge innovation—with unprecedented opportunities for growth and exploration. We believe the latest generation of Llama will ignite new applications and modeling paradigms, including synthetic data generation to enable the improvement and training of smaller models, as well as model distillation—a capability that has never been achieved at this scale in open source.As part of this latest release, we’re introducing upgraded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for doc in llm_output['source_documents']:\n",
    "    md(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Hallucination without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the official LLaMA documentation, the context length for LLaMA 3.1-405B is 4096 tokens. This means that the model can process sequences of up to 4096 tokens (i.e., words or subwords) at a time. However, it's worth noting that the optimal sequence length may vary depending on the specific use case and task."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = llm.invoke(\"how long is the context length in Llama 3.1 405B?\")\n",
    "md(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without RAG, the model generates an incorrect response, and that the user can not verify the information since the sources are not available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
